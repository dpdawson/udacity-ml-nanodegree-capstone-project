{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-candidate",
   "metadata": {},
   "source": [
    "## Overcommit Memory to avoid error\n",
    "\n",
    "!sudo -i\n",
    "\n",
    "echo 1 > /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-attention",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Need to add kaggle credentials to .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle\n",
    "#!kaggle competitions download -c shopee-product-matching -p /tmp\n",
    "#!unzip -q /tmp/shopee-product-matching.zip -d ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-championship",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "\n",
    "### Features and Calculated Statistics\n",
    "\n",
    "### Sampling\n",
    "\n",
    "### Abnormalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in df:\n",
    "    print('\\n')\n",
    "    print(df[attr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posting_id'] = df['posting_id'].str.split('_', expand=True)[1]\n",
    "#df = df.set_index('posting_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_counts = df.image.value_counts().head(100)\n",
    "image_counts.plot.barh(figsize=(15,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_phash_counts = df.image_phash.value_counts().head(100)\n",
    "image_phash_counts.plot.barh(figsize=(15,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = df.title.value_counts().head(100)\n",
    "title_counts.plot.barh(figsize=(15,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_group_counts = df.label_group.value_counts().head(100)\n",
    "label_group_counts.plot.barh(figsize=(15,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "BASE = '../data/train_images/'\n",
    "\n",
    "def displayDF(train, random=False, COLS=6, ROWS=4, path=BASE):\n",
    "    for k in range(ROWS):\n",
    "        plt.figure(figsize=(20,5))\n",
    "        for j in range(COLS):\n",
    "            if random: row = np.random.randint(0,len(train))\n",
    "            else: row = COLS*k + j\n",
    "            name = train.iloc[row,1]\n",
    "            title = train.iloc[row,3]\n",
    "            title_with_return = \"\"\n",
    "            for i,ch in enumerate(title):\n",
    "                title_with_return += ch\n",
    "                if (i!=0)&(i%20==0): title_with_return += '\\n'\n",
    "            img = cv2.imread(path+name)\n",
    "            plt.subplot(1,COLS,j+1)\n",
    "            plt.title(title_with_return)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "        plt.show()\n",
    "        \n",
    "displayDF(df, random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-climate",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-teach",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-challenge",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = sklearn.model_selection.GroupShuffleSplit(test_size=.33, n_splits=1, random_state = random_state)\n",
    "\n",
    "train_idxs, test_idxs = next(sklearn.model_selection.GroupShuffleSplit(test_size=.33, n_splits=1, random_state = random_state).split(df, groups=df['label_group']))\n",
    "\n",
    "train = df.iloc[train_idxs]\n",
    "test = df.iloc[train_idxs]\n",
    "\n",
    "train_idxs, val_idxs = next(sklearn.model_selection.GroupShuffleSplit(test_size=.33, n_splits=1, random_state = random_state).split(df, groups=df['label_group']))\n",
    "\n",
    "train = df.iloc[train_idxs]\n",
    "val = df.iloc[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in train:\n",
    "    print('\\n')\n",
    "    print(df[attr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in test:\n",
    "    print('\\n')\n",
    "    print(df[attr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in val:\n",
    "    print('\\n')\n",
    "    print(df[attr].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['label_group'], axis=1)\n",
    "Y_train = train['label_group']\n",
    "#Y_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "X_test = test.drop(['label_group'], axis=1)\n",
    "Y_test = test['label_group']\n",
    "#Y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "X_val = test.drop(['label_group'], axis=1)\n",
    "Y_val = test['label_group']\n",
    "#Y_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/shopee'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([Y_test, X_test], axis=1).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'shopee'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-rocket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-botswana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-auction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interested-lawsuit",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "### Metrics\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "### Techniques\n",
    "\n",
    "### Complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, column_name='preds'):\n",
    "    def getMetric(col):\n",
    "        def f1score(row):\n",
    "            n = len(np.intersect1d(row.target, row[col]))\n",
    "            return 2 * n / (len(row.target) + len(row[col]))\n",
    "        return f1score\n",
    "\n",
    "\n",
    "    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    df['target'] = df.label_group.map(tmp)\n",
    "    df['f1'] = df.apply(getMetric(column_name), axis=1)\n",
    "    print('CV Score =', df.f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-backing",
   "metadata": {},
   "source": [
    "### Predict Using Phash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = X_train.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "X_train['preds_phash'] = X_train.image_phash.map(tmp)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(Y_train, left_index=True, right_index=True)\n",
    "\n",
    "evaluate_model(train, column_name='preds_phash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-cliff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-pakistan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thrown-newspaper",
   "metadata": {},
   "source": [
    "### Predict Using Title Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = X_train.groupby('title').posting_id.agg('unique').to_dict()\n",
    "X_train['preds_title'] = X_train.title.map(tmp)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(Y_train, left_index=True, right_index=True)\n",
    "\n",
    "evaluate_model(train, column_name='preds_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-style",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-genealogy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-mambo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "international-funeral",
   "metadata": {},
   "source": [
    "### Predict Using Image Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_efficientnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.keras.applications import EfficientNetB0\n",
    "from keras_efficientnets import EfficientNetB0\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, df, img_size=256, batch_size=32, path=BASE): \n",
    "        self.df = df\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.indexes = np.arange( len(self.df) )\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = len(self.df) // self.batch_size\n",
    "        ct += int(( (len(self.df)) % self.batch_size)!=0)\n",
    "        return ct\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indexes)\n",
    "        return X\n",
    "            \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' \n",
    "        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n",
    "        df = self.df.iloc[indexes]\n",
    "        for i,(index,row) in enumerate(df.iterrows()):\n",
    "            img = cv2.imread(self.path+row.image)\n",
    "            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNetB0(weights='imagenet',include_top=False, pooling='avg', input_shape=None)\n",
    "train_gen = DataGenerator(train, batch_size=128)\n",
    "image_embeddings = model.predict(train_gen,verbose=1)\n",
    "print('image embeddings shape is',image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = 50\n",
    "model = NearestNeighbors(n_neighbors=KNN)\n",
    "model.fit(image_embeddings)\n",
    "distances, indices = model.kneighbors(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-observation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-shuttle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-injury",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-there",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blond-payroll",
   "metadata": {},
   "source": [
    "### Predict Using Combined Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_for_cv(row):\n",
    "    x = np.concatenate([row.preds_phash, row.preds_title])\n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(Y_train, left_index=True, right_index=True)\n",
    "train['preds'] = train.apply(combine_for_cv, axis = 1)\n",
    "\n",
    "evaluate_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-rainbow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-cattle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-ballot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-cooler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-extraction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-source",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-actress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "direct-wallpaper",
   "metadata": {},
   "source": [
    "## Model Refinement\n",
    "\n",
    "### Initial Solution\n",
    "\n",
    "### Intermediate Solutions\n",
    "\n",
    "### Final Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-tumor",
   "metadata": {},
   "source": [
    "## Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-comfort",
   "metadata": {},
   "source": [
    "## Model Justification\n",
    "\n",
    "### Benchmark Comparison\n",
    "\n",
    "### Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(hash1, hash2):\n",
    "    return sum([c1 != c2 for c1, c2 in zip(hash1, hash2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "phashes = X_train['image_phash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "preds = []\n",
    "i = 0\n",
    "\n",
    "#a = np.array(phashes)\n",
    "\n",
    "#phash_similarities = (2 * np.inner(a - 0.5, 0.5 - a) + a.shape[1] / 2)\n",
    "\n",
    "#a = None\n",
    "\n",
    "for phash in phashes:\n",
    "    i += 1\n",
    "    phash_similarities = phashes.apply(lambda x: distance.hamming(phash, x))\n",
    "    preds.append(np.where(phash_similarities > 0.9)[0])\n",
    "    break\n",
    "    \n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Y_train.reset_index()\n",
    "tmp[tmp['label_group'] == tmp.iloc[0]['label_group']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0] = np.append(preds[0], 22431)\n",
    "train.iloc[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-swimming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-affair",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-evidence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-binary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-change",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer(stop_words='english', binary=True, max_features=10000)\n",
    "text_embeddings = model.fit_transform(X_train.title).toarray()\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#cosine_similarities = linear_kernel(text_embeddings, text_embeddings)\n",
    "\n",
    "batchsize = 1024\n",
    "cosine_similarities = []\n",
    "for i in range(0, text_embeddings.shape[0], batchsize):\n",
    "    cosine_similarities.extend(linear_kernel(text_embeddings, text_embeddings[i:min(i+batchsize, text_embeddings.shape[0])]).flatten())\n",
    "cosine_similarities = np.array(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=cosine_similarities).to_csv(os.path.join(data_dir, 'train_cosine_similarities.csv'), header=False, index=False)\n",
    "train_cosine_similarities_location = session.upload_data(os.path.join(data_dir, 'train_cosine_similarities.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in range(len(text_embeddings)):\n",
    "    cosine_similarities = linear_kernel(text_embeddings[i:i+1], text_embeddings).flatten()\n",
    "    preds.append(X_train.iloc[np.where(cosine_similarities > 0.7)[0]].posting_id.values)\n",
    "\n",
    "X_train['preds_title'] = preds\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(Y_train, left_index=True, right_index=True)\n",
    "\n",
    "evaluate_model(train, column_name='preds_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-designer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-therapy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-consumption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-creation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-fever",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-johnson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "buf = io.BytesIO()\n",
    "sagemaker.amazon.common.write_numpy_to_dense_tensor(buf, text_embeddings, Y_train.to_numpy())\n",
    "buf.seek(0)\n",
    "\n",
    "s3_train_data = os.path.join(prefix, 'title_text_embeddings.csv')\n",
    "boto3.resource('s3').Bucket(bucket).Object(s3_train_data).upload_fileobj(buf)\n",
    "s3_train_data = f\"s3://{session.default_bucket()}/{prefix}/{'title_text_embeddings.csv'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data,\n",
    "    and return a deployed predictor\n",
    "\n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(\n",
    "        sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "    )\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "\n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {\"train\": s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#s3_train_data = os.path.join(data_dir, 'title_text_embeddings.csv')\n",
    "#boto3.resource('s3').Bucket(bucket).Object(s3_train_data).upload_fileobj(buf)\n",
    "#s3_train_data = f\"s3://{bucket}/{prefix}/train/{key}\"\n",
    "\n",
    "\n",
    "\n",
    "#s3_train_data = os.path.join(data_dir, 'title_text_embeddings.csv')\n",
    "#pd.DataFrame(data=text_embeddings).to_csv(s3_train_data, header=False, index=False)\n",
    "#train_location = session.upload_data(os.path.join(s3_train_data), key_prefix=prefix)\n",
    "\n",
    "hyperparams = {\"feature_dim\": 10000, \"k\": 10, \"sample_size\": 15374, \"predictor_type\": \"classifier\"}\n",
    "output_path = f\"s3://{session.default_bucket()}/{prefix}/output\"\n",
    "\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predictor = knn_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-scheme",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-delta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-cyprus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-stevens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-correction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-penetration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-hindu",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-garlic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-navigation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-graphics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-wallet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = 10\n",
    "model = NearestNeighbors(n_neighbors=KNN)\n",
    "model.fit(text_embeddings)\n",
    "distances, indices = model.kneighbors(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_row_distances(n):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.plot(np.arange(10), np.array(distances[n, ]), 'o-')\n",
    "    plt.title('Text Distance From Train Row %i to Other Train Rows'% n, size=16)\n",
    "    plt.ylabel('Distance to Train Row %i'% n, size=14)\n",
    "    plt.xlabel('Index Sorted by Distance to Train Row %i'% n, size=14)\n",
    "    plt.show()\n",
    "\n",
    "    print(X_train.loc[np.array(indices[n, :10]), ['title']].merge(Y_train.loc[np.array(indices[n, :10])], left_index=True, right_index=True))\n",
    "    \n",
    "plot_row_distances(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    preds.append(X_train.loc[np.where(distances[i] < 0.7)[0]].posting_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['preds_title'] = preds\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(Y_train, left_index=True, right_index=True)\n",
    "\n",
    "evaluate_model(train, column_name='preds_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "cosine_similarities = linear_kernel(text_embeddings[0:1], text_embeddings).flatten()\n",
    "related_docs_indices = np.where(cosine_similarities > 0.7)[0]\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp['indices'] = related_docs_indices\n",
    "tmp['similarities'] = cosine_similarities[cosine_similarities > 0.7]\n",
    "\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "cosine_similarities = linear_kernel(text_embeddings[0:1], text_embeddings).flatten()\n",
    "np.where(cosine_similarities > 0.7)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    preds.append(X_train.loc[np.where(distances[i] < 0.7)[0]].posting_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(text_embeddings[0:1], text_embeddings).flatten()\n",
    "below_threshold_indices = cosine_similarities < 0.7\n",
    "cosine_similarities[below_threshold_indices] = 0\n",
    "related_docs_indices = cosine_similarities.argsort()[:-51:-1]\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp['indices'] = related_docs_indices\n",
    "tmp['similarities'] = cosine_similarities[related_docs_indices]\n",
    "\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(text_embeddings, text_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-approval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-opportunity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-reform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-cornwall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-expansion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-ghana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-nevada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-boost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-mathematics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
